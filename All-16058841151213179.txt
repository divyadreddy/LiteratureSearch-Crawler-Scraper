{"0": {"type": "inproceedings", "author": "Gopinath, Divya and Khurshid, Sarfraz and Saha, Diptikalyan and Chandra, Satish", "title": "Data-guided repair of selection statements", "year": 2014, "isbn": "9781450327565", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/2568225.2568303", "doi": "10.1145/2568225.2568303", "abstract": " Database-centric programs form the backbone of many enterprise systems. Fixing defects in such programs takes much human effort due to the interplay between imperative code and database-centric logic. This paper presents a novel data-driven approach for automated fixing of bugs in the selection condition of database statements (e.g., WHERE clause of SELECT statements) \u2013 a common form of bugs in such programs. Our key observation is that in real-world data, there is information latent in the distribution of data that can be useful to repair selection conditions efficiently. Given a faulty database program and input data, only a part of which induces the defect, our novelty is in determining the correct behavior for the defect-inducing data by taking advantage of the information revealed by the rest of the data. We accomplish this by employing semi-supervised learning to predict the correct behavior for defect-inducing data and by patching up any inaccuracies in the prediction by a SAT-based combinatorial search. Next, we learn a compact decision tree for the correct behavior, including the correct behavior on the defect-inducing data. This tree suggests a plausible fix to the selection condition. We demonstrate the feasibility of our approach on seven realworld examples. ", "booktitle": "Proceedings of the 36th International Conference on Software Engineering", "pages": "243-253", "numpages": "11", "keywords": "Databases, SAT, data-centric programs, ABAP, Program Repair, Support Vector Machines, Machine Learning", "location": "Hyderabad, India", "series": "ICSE 2014"}, "1": {"type": "inproceedings", "author": "Misra, Janardan and Sengupta, Shubhashis and Rawat, Divya and Savagaonkar, Milind and Podder, Sanjay", "title": "Data-driven application maintenance: experience from the trenches", "year": 2017, "isbn": "9781538627976", "publisher": "IEEE Press", "url": "https://doi.org/10.1109/SER-IP.2017..8", "doi": "10.1109/SER-IP.2017..8", "abstract": "In this paper we present our experience during design, development, and pilot deployments of a data-driven machine learning based application maintenance solution. We implemented a proof of concept to address a spectrum of interrelated problems encountered in application maintenance projects including duplicate incident ticket identification, assignee recommendation, theme mining, and mapping of incidents to business processes. In the context of IT services, these problems are frequently encountered, yet there is a gap in bringing automation and optimization. Despite long-standing research around mining and analysis of software repositories, such research outputs are not adopted well in practice due to the constraints these solutions impose on the users. We discuss need for designing pragmatic solutions with low barriers to adoption and addressing right level of complexity of problems with respect to underlying business constraints and nature of data.", "booktitle": "Proceedings of the 4th International Workshop on Software Engineering Research and Industrial Practice", "pages": "48-54", "numpages": "7", "keywords": "assignee recommendation, machine learning, business process mapping, application maintenance, theme mining, incident management, text analysis, duplicate bug identification", "location": "Buenos Aires, Argentina", "series": "SER&amp;IP '17"}, "2": {"type": "inproceedings", "author": "Piplani, Divya and Singh, Dinesh Kumar and Srinivasan, Karthik and Ramesh, N. and Kumar, Anil and kumar, Viswa", "title": "Digital Platform for Data Driven Aquaculture Farm Management", "year": 2015, "isbn": "9781450340533", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/2835966.2836277", "doi": "10.1145/2835966.2836277", "abstract": "Besides meeting the domestic needs of cheap animal protein, Indian fisheries, is source of livelihood for 14.5 million fishers [1]. During FY2014-15, inland fisheries grew at 7.9%, fetching US $5.5 billion in foreign exchange [2]. But aquaculture farming requires lot of care, including periodic observations of the weather, water quality and feed consumption. Drop in feed consumption, coupled with low temperature, may be an early indication of a disease. In FY13-14, shrimp production fell in Southeast Asian countries due to spread of Early Mortality Syndrome (EMS) disease, reducing export by 50% [3]. Hence farm data is crucial for daily data driven crop health monitoring and management. But it's very difficult to manually assimilate such bulky data and extract information impacting real-time decision making. This is especially challenging when each farmer manages multiple ponds, spread out over a distance and with no or low speed data network.mKRISHI\u00ae collaborated with farm managers, government regulators and farmers to develop \"mKRISHI\u00ae -AQUA\" service, in an iterative, multi-phase development process. This service helps in data collection, compilation and presentation of the patterns in visual format, enabling decision on further operations (such as feeding) in a more real-time manner compared to paper based operation.", "booktitle": "Proceedings of the 7th International Conference on HCI, IndiaHCI 2015", "pages": "95-101", "numpages": "7", "keywords": "mKRISHI AQUA, Mobile, Shrimp, ICT, Aquaculture, MPEDA", "location": "Guwahati, India", "series": "IndiaHCI'15"}, "3": {"type": "inproceedings", "author": "Kumar, Raj and Gupta, Divya", "title": "Security in real time multimedia data based on generalized keys", "year": 2011, "isbn": "9781450306355", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/2007052.2007071", "doi": "10.1145/2007052.2007071", "abstract": "In this paper, address the problem of encryption/decryption regions of interest in a video sequence for the purpose of security in video data. The proposed an efficient solution based on 3D encryption/decryption based. More specifically the videos files breaks different frames as 2D based digital images, after 2D based images encrypt/decrypt by the 2D generalized algorithms. The simulation results show that the mechanism can be successfully applied to obscure information in regions of the interest in the scene which provides the different level of security. Further, the keys values are flexible and allow choosing different types for security purpose.", "booktitle": "Proceedings of the International Conference on Advances in Computing and Artificial Intelligence", "pages": "93-96", "numpages": "4", "keywords": "gray code, 3-D, process, encryption/decryption", "location": "Rajpura/Punjab, India", "series": "ACAI '11"}, "4": {"type": "inproceedings", "author": "Wang, Bin and Chan, Cy and Somasi, Divya and Macfarlane, Jane and Rask, Eric", "title": "Data-Driven Energy Use Estimation in Large Scale Transportation Networks", "year": 2019, "isbn": "9781450369787", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/3357492.3358632", "doi": "10.1145/3357492.3358632", "abstract": "Energy consumption in the transportation sector accounts for 28.8% of the total value among all the industry sectors in the United States, reaching 28.2 quadrillion btu in 2017. Having an accurate evaluation of the vehicle fuel and energy consumption values is a challenging task due to numerous implicit influential factors, such as the variety of powertrain configurations, time-varying traffic and congestion patterns, and emerging new technologies, such as regenerative braking. In this paper, we propose to present a data-driven computational framework to evaluate the energy impact on the transportation system at different scales, leveraging the scalable high-performance transportation simulator, Mobiliti. Instead of using empirical energy models, we create a deep-neural-network mapping between the fuel and energy consumption rate with a variety of heterogeneous driving conditions based on dynamometer test datasets, real-world drive cycle survey datasets as well as real-world GPS probe datasets. For the dynamic driving behaviors, machine learning algorithms are applied over the real-world drive cycle datasets to identify the dominant features and to cluster the drive cycles into representative groups, which can be used to generate high-resolution random drive cycles using a Markov chain approach. Using Mobiliti, both urban-scale static evaluations and dynamic analysis at the trip level can be estimated with a significantly improved fidelity. We demonstrate this approach through case studies with different scales and varied penetrations of different vehicle types, such as conventional ICE vehicles, hybrid vehicles, and electric vehicles.", "booktitle": "Proceedings of the 2nd ACM/EIGSCC Symposium on Smart Cities and Communities", "pages": "1-6", "numpages": "6", "keywords": "drive cycles, data-driven, vehicle energy model, transportation simulation", "location": "Portland, OR, USA", "series": "SCC '19"}, "5": {"type": "inproceedings", "author": "Arora, Divya and Raghunathan, Anand and Ravi, Srivaths and Jha, Niraj K.", "title": "Enhancing security through hardware-assisted run-time validation of program data properties", "year": 2005, "isbn": "1595931619", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/1084834.1084884", "doi": "10.1145/1084834.1084884", "abstract": "The growing number of information security breaches in electronic and computing systems calls for new design paradigms that consider security as a primary design objective. This is particularly relevant in the embedded domain, where the security solution should be customized to the needs of the target system, while considering other design objectives such as cost, performance, and power. Due to the increasing complexity and shrinking design cycles of embedded software, most embedded systems present a host of software vulnerabilities that can be exploited by security attacks. Many attacks are initiated by causing a violation in the properties of data ( e.g., integrity, privacy, access control rules, etc.) associated with a \"trusted\" program that is executing on the system, leading to a range of undesirable effects.In this work, we develop a general framework that provides security assurance against a wide class of security attacks. Our work is based on the observation that a program's permissible behaviorwith respect to data accesses can be characterized by certain properties. We present a hardware/software approach wherein such properties can be encoded as data attributes and enforced as security policies during program execution. These policies may be application-specific (e.g., access control for certain data structures), compiler-generated (e.g., enforcing that variables are accessed only within their scope), or universally applicable to all programs (e.g., disallowing writes to unallocated memory). We show how an embedded system architecture can support such policies by (i) enhancing the memory hierarchy to represent the attributes of each datum as security tags that are linked to it through its lifetime, and (ii) adding a configurable hardware checker that interprets the semantics of the tags and enforces the desired security policies. We evaluated the effectiveness of the proposed architecture in enforcing various security policies for several embedded benchmarks. Our experiments in the context of the Simplescalar framework demonstrate that the proposed solution ensures run-time validation of program data properties with minimal execution time overheads.", "booktitle": "Proceedings of the 3rd IEEE/ACM/IFIP international conference on Hardware/software codesign and system synthesis", "pages": "190-195", "numpages": "6", "keywords": "run-time checks, secure architectures, data tagging", "location": "Jersey City, NJ, USA", "series": "CODES+ISSS '05"}, "6": {"type": "inproceedings", "author": "Chaudhary, Megha and Bansal, Aneesh and Bansal, Divya and Raman, Bhaskaran and Ramakrishnan, K. K. and Aggarwal, Naveen", "title": "Finding occupancy in buses using crowdsourced data from smartphones", "year": 2016, "isbn": "9781450340328", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/2833312.2833460", "doi": "10.1145/2833312.2833460", "abstract": "In the present scenario, developing countries like India are facing huge traffic congestion problems. Commuters have to wait long hours for arrival of buses, and when the bus arrives it is often found to be overcrowded, causing inconvenience in the commuters and discouraging them to use public transit system. The ITS(Intelligent Transport System) developed so far does provide arrival time of buses in real time but such systems are rare which provide the passenger occupancy in real time. Most of such installations use extortionate devices like passenger counting devices, cameras etc installed on the buses and at the bus stops. In this paper we propose a cost effective user participation based mode of collecting information about occupancy level of public transportation system using the potential of smartphones. Smartphones have inbuilt sensors like GPS which can be used to extract locational intelligence of the commuters. Hence, information gets crowdsourced from commuters and they themselves can provide information about occupancy level of a bus using their smartphones. The information so collected is stored in a historical database which is analyzed and processed to obtain occupancy level patterns for different routes on different days. The patterns observed are used to make predictions of occupancy level in a bus. Our results show that it is possible to achieve an accuracy to a level of 91.86 percent.", "booktitle": "Proceedings of the 17th International Conference on Distributed Computing and Networking", "pages": "1-4", "numpages": "4", "keywords": "ITS (intelligent transportation system), APC (automatic passenger counting system), AVL (automatic vehicle location system), crowdsourcing", "location": "Singapore, Singapore", "series": "ICDCN '16"}, "7": {"type": "inproceedings", "author": "Papagiannis, Ioannis and Watcharapichat, Pijika and Muthukumaran, Divya and Pietzuch, Peter", "title": "BrowserFlow: Imprecise Data Flow Tracking to Prevent Accidental Data Disclosure", "year": 2016, "isbn": "9781450343008", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/2988336.2988345", "doi": "10.1145/2988336.2988345", "abstract": "With the use of external cloud services such as Google Docs or Evernote in an enterprise setting, the loss of control over sensitive data becomes a major concern for organisations. It is typical for regular users to violate data disclosure policies accidentally, e.g. when sharing text between documents in browser tabs. Our goal is to help such users comply with data disclosure policies: we want to alert them about potentially unauthorised data disclosure from trusted to untrusted cloud services. This is particularly challenging when users can modify data in arbitrary ways, they employ multiple cloud services, and cloud services cannot be changed.To track the propagation of text data robustly across cloud services, we introduce imprecise data flow tracking, which identifies data flows implicitly by detecting and quantifying the similarity between text fragments. To reason about violations of data disclosure policies, we describe a new text disclosure model that, based on similarity, associates text fragments in web browsers with security tags and identifies unauthorised data flows to untrusted services. We demonstrate the applicability of imprecise data tracking through BrowserFlow, a browser-based middleware that alerts users when they expose potentially sensitive text to an untrusted cloud service. Our experiments show that BrowserFlow can robustly track data flows and manage security tags for documents with no noticeable performance impact.", "booktitle": "Proceedings of the 17th International Middleware Conference", "pages": "1-13", "numpages": "13", "keywords": "Data disclosure, data tracking, cloud security, browser-based middleware", "location": "Trento, Italy", "series": "Middleware '16"}, "8": {"type": "inproceedings", "author": "Priebe, Christian and Muthukumaran, Divya and O' Keeffe, Dan and Eyers, David and Shand, Brian and Kapitza, Ruediger and Pietzuch, Peter", "title": "CloudSafetyNet: Detecting Data Leakage between Cloud Tenants", "year": 2014, "isbn": "9781450332392", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/2664168.2664174", "doi": "10.1145/2664168.2664174", "abstract": "When tenants deploy applications under the control of third-party cloud providers, they must trust the provider's security mechanisms for inter-tenant isolation, resource sharing and access control. Despite a provider's best efforts, accidental data leakage may occur due to misconfigurations or bugs in the cloud platform. Especially in Platform-as-a-Service (PaaS) clouds, which rely on weaker forms of isolation, the potential for unnoticed data leakage is high. Prior work to raise tenants' trust in clouds relies on attestation, which limits the management flexibility of providers, or fine-grained data tracking, which has high overheads.We describe CloudSafetyNet (CSN), a lightweight monitoring framework that gives tenants visibility into the propagation of their application data in a cloud environment with low performance overhead. It exploits the incentive of tenants to co-operate with each other to detect accidental data leakage. CSN transparently adds opaque security tags to a subset of form fields in HTTP requests, using a client-side JavaScript library. Socket-level monitors maintain a log of observed tags flowing between application components. Tenants retrieve their logs and identify foreign tags that indicate data leakage. To check the correct operation of CSN, tenants send probe requests with known tags and verify that monitors are logging correctly. Using an implementation of CSN deployed on the OpenShift and AppScale PaaS platforms, we show that it can discover misconfigurations and bugs with a negligible performance impact.", "booktitle": "Proceedings of the 6th edition of the ACM Workshop on Cloud Computing Security", "pages": "117-128", "numpages": "12", "keywords": "socket-level monitoring, inter-tenant isolation, data leakage detection, cloud", "location": "Scottsdale, Arizona, USA", "series": "CCSW '14"}, "9": {"type": "inproceedings", "author": "Saxena, Divya and Raychoudhury, Vaskar and SriMahathi, Nalluri", "title": "<i>SmartHealth-NDNoT</i>: Named Data Network of Things for Healthcare Services", "year": 2015, "isbn": "9781450335256", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/2757290.2757300", "doi": "10.1145/2757290.2757300", "abstract": "In recent years, healthcare sector has emerged as a major application area of Internet-of-Things (IoT). IoT aims to automate healthcare services through remote monitoring of patients using several vital sign sensors. Remotely collected patient records are then conveyed to the hospital servers through the user's smartphones. Healthcare IoT can thus reduce a lot of overhead while allowing people to access healthcare services all the time and everywhere. However, healthcare IoT exchanges data over the IP-centric Internet which has vulnerabilities related to security, privacy, and mobility. Those features are added to the Internet as external add-ons. In order to solve this problem, in this paper, we propose to use Named Data Networking (NDN), which is a future Internet paradigm based on Content-Centric Networking (CCN). NDN has in-built support for user mobility which is well-suited for mobile patients and caregivers. NDN also ensures data security instead of channel security earlier provided by the Internet. In this paper, we have developed NDNoT, which is an IoT solution for smart mobile healthcare using NDN. Our proof-of-concept prototype shows the usability of our proposed architecture.", "booktitle": "Proceedings of the 2015 Workshop on Pervasive Wireless Healthcare", "pages": "45-50", "numpages": "6", "keywords": "open mhealth architecture, named data networking (ndn), healthcare, ndnot, internet of things (iot)", "location": "Hangzhou, China", "series": "MobileHealth '15"}, "10": {"type": "inproceedings", "author": "Muthukumaran, Divya and O'Keeffe, Dan and Priebe, Christian and Eyers, David and Shand, Brian and Pietzuch, Peter", "title": "FlowWatcher: Defending against Data Disclosure Vulnerabilities in Web Applications", "year": 2015, "isbn": "9781450338325", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/2810103.2813639", "doi": "10.1145/2810103.2813639", "abstract": "Bugs in the authorisation logic of web applications can expose the data of one user to another. Such data disclosure vulnerabilities are common---they can be caused by a single omitted access control check in the application. We make the observation that, while the implementation of the authorisation logic is complex and therefore error-prone, most web applications only use simple access control models, in which each piece of data is accessible by a user or a group of users. This makes it possible to validate the correct operation of the authorisation logic externally, based on the observed data in HTTP traffic to and from an application.We describe FlowWatcher, an HTTP proxy that mitigates data disclosure vulnerabilities in unmodified web applications. FlowWatcher monitors HTTP traffic and shadows part of an application's access control state based on a rule-based specification of the user-data-access (UDA) policy. The UDA policy states the intended data ownership and how it changes based on observed HTTP requests. FlowWatcher detects violations of the UDA policy by tracking data items that are likely to be unique across HTTP requests and responses of different users. We evaluate a prototype implementation of FlowWatcher as a plug-in for the Nginx reverse proxy and show that, with short UDA policies, it can mitigate CVE bugs in six~popular web applications.", "booktitle": "Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security", "pages": "603-615", "numpages": "13", "keywords": "http proxy, policy, data disclosure, web application security", "location": "Denver, Colorado, USA", "series": "CCS '15"}, "11": {"type": "inproceedings", "author": "Tomar, Divya and Agarwal, Sonali", "title": "Direct acyclic graph based multi-class twin support vector machine for pattern classification", "year": 2015, "isbn": "9781450334365", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/2732587.2732598", "doi": "10.1145/2732587.2732598", "abstract": "In this paper, we propose a novel Multi-class Twin Support Vector Machine (MTWSVM) classifier on the basis of Direct Acyclic Graph (DAG) approach. MTWSVM is the multi-class extension of the recently proposed binary Twin Support Vector Machine (TWSVM) classifier. The optimization problems of the proposed classifier are solved by the Successive Over Relaxation (SOR) technique which speed up the training phase. The performance of the proposed classifier is compared with the existing approaches and validated against ten benchmark datasets. Further, we have investigated the efficiency of proposed classifier for Handwritten Digits recognition application. The effectiveness of the proposed classifier over existing approaches is demonstrated with the help of experimental results.", "booktitle": "Proceedings of the Second ACM IKDD Conference on Data Sciences", "pages": "80-85", "numpages": "6", "keywords": "multi-class twin support vector machine, direct acyclic graph, successive over relaxation handwritten digits, twin support vector machine", "location": "Bangalore, India", "series": "CoDS '15"}, "12": {"type": "article", "author": "Pandove, Divya and Goel, Shivan and Rani, Rinkl", "title": "Systematic Review of Clustering High-Dimensional and Large Datasets", "year": 2018, "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/3132088", "doi": "10.1145/3132088", "abstract": "Technological advancement has enabled us to store and process huge amount of data in relatively short spans of time. The nature of data is rapidly changing, particularly its dimensionality is more commonly multi- and high-dimensional. There is an immediate need to expand our focus to include analysis of high-dimensional and large datasets. Data analysis is becoming a mammoth task, due to incremental increase in data volume and complexity in terms of heterogony of data. It is due to this dynamic computing environment that the existing techniques either need to be modified or discarded to handle new data in multiple high-dimensions. Data clustering is a tool that is used in many disciplines, including data mining, so that meaningful knowledge can be extracted from seemingly unstructured data. The aim of this article is to understand the problem of clustering and various approaches addressing this problem. This article discusses the process of clustering from both microviews (data treating) and macroviews (overall clustering process). Different distance and similarity measures, which form the cornerstone of effective data clustering, are also identified. Further, an in-depth analysis of different clustering approaches focused on data mining, dealing with large-scale datasets is given. These approaches are comprehensively compared to bring out a clear differentiation among them. This article also surveys the problem of high-dimensional data and the existing approaches, that makes it more relevant. It also explores the latest trends in cluster analysis, and the real-life applications of this concept. This survey is exhaustive as it tries to cover all the aspects of clustering in the field of data mining.", "booktitle": "ACM Trans. Knowl. Discov. Data", "pages": "1-68", "numpages": "68", "keywords": "dimensionality reduction, data clustering process, clustering tendency, large scale data mining, Cluster analysis, data clustering applications"}, "13": {"type": "inproceedings", "author": "Sharma, Ankur and Schuhknecht, Felix Martin and Agrawal, Divya and Dittrich, Jens", "title": "Blurring the Lines between Blockchains and Database Systems: the Case of Hyperledger Fabric", "year": 2019, "isbn": "9781450356435", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/3299869.3319883", "doi": "10.1145/3299869.3319883", "abstract": "Within the last few years, a countless number of blockchain systems have emerged on the market, each one claiming to revolutionize the way of distributed transaction processing in one way or the other. Many blockchain features, such as byzantine fault tolerance, are indeed valuable additions in modern environments. However, despite all the hype around the technology, many of the challenges that blockchain systems have to face are fundamental transaction management problems. These are largely shared with traditional database systems, which have been around for decades already. These similarities become especially visible for systems, that blur the lines between blockchain systems and classical database systems. A great example of this is Hyperledger Fabric, an open-source permissioned blockchain system under development by IBM. By implementing parallel transaction processing, Fabric's workflow is highly motivated by optimistic concurrency control mechanisms in classical database systems. This raises two questions: (1)~Which conceptual similarities and differences do actually exist between a system such as Fabric and a classical distributed database system? (2)~Is it possible to improve on the performance of Fabric by transitioning technology from the database world to blockchains and thus blurring the lines between these two types of systems even further? To tackle these questions, we first explore Fabric from the perspective of database research, where we observe weaknesses in the transaction pipeline. We then solve these issues by transitioning well-understood database concepts to Fabric, namely transaction reordering as well as early transaction abort. Our experimental evaluation under the Smallbank benchmark as well as under a custom workload shows that our improved version Fabric++ significantly increases the throughput of successful transactions over the vanilla version by up to a factor of 12x, while decreasing the average latency to almost half.", "booktitle": "Proceedings of the 2019 International Conference on Management of Data", "pages": "105-122", "numpages": "18", "keywords": "information systems, distributed systems, ledger, concurrenct control, fabric, hyperledger, blockchain, distributed ledger, transactions", "location": "Amsterdam, Netherlands", "series": "SIGMOD '19"}, "14": {"type": "inproceedings", "author": "Gao, Xiangyu and Kim, Taegyun and Wong, Michael D. and Raghunathan, Divya and Varma, Aatish Kishan and Kannan, Pravein Govindan and Sivaraman, Anirudh and Narayana, Srinivas and Gupta, Aarti", "title": "Switch Code Generation Using Program Synthesis", "year": 2020, "isbn": "9781450379557", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/3387514.3405852", "doi": "10.1145/3387514.3405852", "abstract": "Writing packet-processing programs for programmable switch pipelines is challenging because of their all-or-nothing nature: a program either runs at line rate if it can fit within pipeline resources, or does not run at all. It is the compiler's responsibility to fit programs into pipeline resources. However, switch compilers, which use rewrite rules to generate switch machine code, often reject programs because the rules fail to transform programs into a form that can be mapped to a pipeline's limited resources---even if a mapping actually exists.This paper presents a compiler, Chipmunk, which formulates code generation as a program synthesis problem. Chipmunk uses a program synthesis engine, SKETCH, to transform high-level programs down to switch machine code. However, naively formulating code generation as program synthesis can lead to long compile times. Hence, we develop a new domain-specific synthesis technique, slicing, which reduces compile times by 1-387x and 51x on average.Using a switch hardware simulator, we show that Chipmunk compiles many programs that a previous rule-based compiler, Domino, rejects. Chipmunk also produces machine code with fewer pipeline stages than Domino. A Chipmunk backend for the Tofino programmable switch shows that program synthesis can produce machine code for high-speed switches.", "booktitle": "Proceedings of the Annual conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication", "pages": "44-61", "numpages": "18", "keywords": "slicing, program synthesis, packet processing pipelines, code generation, Programmable switches", "location": "Virtual Event, USA", "series": "SIGCOMM '20"}, "15": {"type": "@INPROCEEDINGS", "id": "8738596,", "author": "Y. {Wadadekar} and L. T. {George} and B. P. {Ratna Kumar} and I. {Chandra} and D. {Oberoi", "booktitle": "2019 URSI Asia-Pacific Radio Science Conference (AP-RASC)", "title": "The GMRT Archival Data Processing Project", "year": "2019", "volume": "", "number": "", "pages": "1-1", "abstract": "The GMRT Online Archive houses over 80 terabytes of interferometric observations obtained with the GMRT, since the first public observing cycle in 2002. The utility of this vast archive of raw UV visibilities, likely the largest of any Indian telescope, can be significantly enhanced if first look (and where possible, science ready) processed images can be made available to the user community. We have initiated a project to pipeline process GMRT images in the 150, 240, 325 and 610 MHz bands. The SPAM pipeline developed by Huib Intema is being used for this purpose.", "keywords": "astronomical telescopes;data acquisition;information retrieval systems;pipeline processing;GMRT archival data processing project;GMRT Online Archive houses;interferometric observations;public observing cycle;Indian telescope;pipeline process GMRT images;UV visibilities;Huib Intema;frequency 150.0 MHz;frequency 240.0 MHz;frequency 325.0 MHz;frequency 610.0 MHz;Pipelines;Data processing;Telescopes;Extraterrestrial measurements;Astrophysics;Radio interferometry;Time measurement", "doi": "10.23919/URSIAP-RASC.2019.8738596", "ISSN": "", "month": "March"}, "16": {"type": "@INPROCEEDINGS", "id": "6949264,", "author": "D. {Wadhwa} and P. {Dabas", "booktitle": "2014 5th International Conference - Confluence The Next Generation Information Technology Summit (Confluence)", "title": "A coherent dynamic remote integrity check on cloud data utilizing homomorphic cryptosystem", "year": "2014", "volume": "", "number": "", "pages": "91-96", "abstract": "Checking remote data integrity in climacteric cloud computing infrastructure is a valuable matter of concern. As the idea of cloud computing entered into a wide implementation today, data access becomes a major security issue. One of the various privacy concerns that are possibly taken into consideration relates to the maintenance of cloud data integrity. Directing users to check data integrity under public audibility is a task to be greatly considered. It is made through third party verifier who provides the client a proof whether the data placed on the server is altered or not. We proposed a dynamic data integrity checking mechanism in which the proof of correct data possession can be made from server on demand. Verifier, on behalf of the client can make a call to the server for verifying the correctness of the stored data, at anytime. This protocol is designed keeping the dynamic nature of cloud as the data placed on the server goes on changing very frequently. Thus, a dynamic data integrity approach is adopted here which includes the RSA encryption system as a method for public cryptosystem. A multiplicative homomorphic property, an idea towards integrity checking of cloud data, is implemented here. The beauty of applying and including the homomorphism property in our protocol is that, the proof can be generated by the third party verifier without having any clue of the original data. Our research is dually aimed at A) generating proof of correct data possession in a dynamic cloud environment B) providing high security of cloud data through homomorphic cryptosystem. The proposed technique is implemented in a very productive and cost effective manner. The testing results of the proposed work are propitious and favorable.", "keywords": "cloud computing;cryptographic protocols;data integrity;public key cryptography;proof generation;third party verifier;multiplicative homomorphic property;public cryptosystem;RSA encryption system;stored data correctness verification;server on demand;correct data possession proof;dynamic data integrity checking mechanism;climacteric cloud computing infrastructure;homomorphic cryptosystem;cloud data;coherent dynamic remote integrity check;Servers;Cloud computing;Protocols;Educational institutions;Ciphers;cloud computing;integrity checking;homomorphism;verifier;data possesion;RSA cryptosystem;dynamic cloud data", "doi": "10.1109/CONFLUENCE.2014.6949264", "ISSN": "", "month": "Sep."}, "17": {"type": "@INPROCEEDINGS", "id": "7530186,", "author": "D. M. {Menon} and N. {Radhika", "booktitle": "2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)", "title": "Anomaly detection in smart grid traffic data for home area network", "year": "2016", "volume": "", "number": "", "pages": "1-4", "abstract": "Strengthening of Smart Grid functionalities has become the need of the 21st Century. Security evolves to be the primary concern at the deployment level of Smart Grids. Cyber security threats and vulnerabilities in Smart grid Network needs to be addressed before the deployment of the Smart Grid. Our proposed intrusion detection scheme identifies anomalies in the Smart Grid traffic and detects attacks like flooding which causes Denial of Service in Smart Grid Networks. This paper applies k-Means algorithm for clustering of traffic data and outlier detection for the data transmitted between utility Centre and the Smart Homes. Performance of the algorithm has been compared with other clustering algorithms and the results were found to have higher percentage in anomaly detection.", "keywords": "computer network security;home computing;home networks;pattern clustering;smart power grids;anomaly detection;smart grid traffic data clustering;home area network;smart grid functionalities;cyber security threats;intrusion detection;denial of service;smart grid networks;k-means algorithm;utility centre;smart homes;clustering algorithms;Smart grids;Floods;Clustering algorithms;Intrusion detection;Computers;Smart meters;Smart Grid;Anomaly Detection;k-Means Clustering", "doi": "10.1109/ICCPCT.2016.7530186", "ISSN": "", "month": "March"}}